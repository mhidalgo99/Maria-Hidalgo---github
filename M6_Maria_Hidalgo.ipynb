{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio Módulo 6 (diamonds.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar del dataset con Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, MinMaxScaler, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/28 09:35:21 WARN Utils: Your hostname, MacBook-Air-de-Maria.local resolves to a loopback address: 127.0.0.1; using 192.168.0.22 instead (on interface en0)\n",
      "25/02/28 09:35:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/28 09:35:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
      "| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
      "| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|\n",
      "| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|\n",
      "| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|\n",
      "| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|\n",
      "| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|\n",
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DiamondsAnalysis\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"carat\", FloatType(), True),  \n",
    "    StructField(\"cut\", StringType(), True), \n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"clarity\", StringType(), True),\n",
    "    StructField(\"depth\", FloatType(), True),  \n",
    "    StructField(\"table\", FloatType(), True),   \n",
    "    StructField(\"price\", IntegerType(), True), \n",
    "    StructField(\"x\", FloatType(), True),      \n",
    "    StructField(\"y\", FloatType(), True),      \n",
    "    StructField(\"z\", FloatType(), True),\n",
    "])\n",
    "\n",
    "file_path = \"/Users/mariahidalgo/Desktop/Maria Hidalgo - github/Data/diamonds.csv\"\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, schema=schema)\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline regresión 'price' con preprocesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Preprocesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, MinMaxScaler, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(inputCols=['carat', 'depth', 'table', 'x', 'y', 'z', 'price'], \n",
    "                  outputCols=['carat_imputed', 'depth_imputed', 'table_imputed', 'x_imputed', 'y_imputed', 'z_imputed', 'price_imputed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_cut = StringIndexer(inputCol=\"cut\", outputCol=\"cut_index\")\n",
    "encoder_cut = OneHotEncoder(inputCol=\"cut_index\", outputCol=\"cut_encoded\")\n",
    "\n",
    "indexer_color = StringIndexer(inputCol=\"color\", outputCol=\"color_index\")\n",
    "encoder_color = OneHotEncoder(inputCol=\"color_index\", outputCol=\"color_encoded\")\n",
    "\n",
    "indexer_clarity = StringIndexer(inputCol=\"clarity\", outputCol=\"clarity_index\")\n",
    "encoder_clarity = OneHotEncoder(inputCol=\"clarity_index\", outputCol=\"clarity_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_features = VectorAssembler(\n",
    "    inputCols=[\"carat_imputed\", \"depth_imputed\", \"table_imputed\", \"x_imputed\", \"y_imputed\", \"z_imputed\"],\n",
    "    outputCol=\"features_temp\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_temp\", outputCol=\"features_scaled\", withMean=True, withStd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[\"features_scaled\", \"cut_encoded\", \"color_encoded\", \"clarity_encoded\"],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price_imputed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Creación del pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    imputer,\n",
    "    indexer_cut, encoder_cut,\n",
    "    indexer_color, encoder_color,\n",
    "    indexer_clarity, encoder_clarity,\n",
    "    assembler_features, scaler,\n",
    "    final_assembler,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/28 09:57:07 WARN Instrumentation: [a9d71a45] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/02/28 09:57:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/02/28 09:57:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model = pipeline.fit(train_data)\n",
    "predictions = model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1148.2259293250036\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"price_imputed\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline clasificación multiclase sobre variable 'cut' con preprocesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Preprocesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, MinMaxScaler, StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(inputCols=[\"carat\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\"], \n",
    "                  outputCols=[\"carat_imputed\", \"depth_imputed\", \"table_imputed\", \n",
    "                              \"price_imputed\", \"x_imputed\", \"y_imputed\", \"z_imputed\"]).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_cut = StringIndexer(inputCol=\"cut\", outputCol=\"cut_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_color = StringIndexer(inputCol=\"color\", outputCol=\"color_indexed\")\n",
    "indexer_clarity = StringIndexer(inputCol=\"clarity\", outputCol=\"clarity_indexed\")\n",
    "\n",
    "encoder_color = OneHotEncoder(inputCol=\"color_indexed\", outputCol=\"color_encoded\")\n",
    "encoder_clarity = OneHotEncoder(inputCol=\"clarity_indexed\", outputCol=\"clarity_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"carat_imputed\", \"depth_imputed\", \"table_imputed\",\n",
    "                                       \"price_imputed\", \"x_imputed\", \"y_imputed\", \"z_imputed\"],\n",
    "                            outputCol=\"features_assembled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features_assembled\", outputCol=\"features_scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_assembler = VectorAssembler(inputCols=[\"features_scaled\", \"color_encoded\", \"clarity_encoded\"], \n",
    "                                  outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Modelo de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"cut_indexed\", numTrees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[imputer, indexer_cut, indexer_color, indexer_clarity,\n",
    "                            encoder_color, encoder_clarity, assembler, scaler, \n",
    "                            final_assembler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6924564796905223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"cut_indexed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gridsearch con CrossValidation sobre cualquiera de los pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(classifier.numTrees, [50, 100, 150]) \\\n",
    "    .addGrid(classifier.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(labelCol=\"cut_indexed\", predictionCol=\"prediction\", metricName=\"accuracy\"), \n",
    "    numFolds=3, \n",
    "    parallelism=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nota: Hola Alan, este último apartado genera un error que indica que Spark no tiene suficiente almacenamiento, creo que nunca antes me ha surgido un error similiar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
